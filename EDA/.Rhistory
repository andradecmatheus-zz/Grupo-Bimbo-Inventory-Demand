labs(title = paste('Bar plot for variable:', col)) +
#scale_y_continuous(labels=function(x)paste(x/1000, 'k')) +
theme_bw()
}
# create boxPlot function
boxPlot <- function(col, data) {
data %>%
ggplot(aes(x = counts)) +
geom_boxplot(fill = '#9D7660', color = '#634432', alpha = 0.7) +
theme(axis.text.y = element_blank()) +
xlab(paste(col, 'frequency')) +
scale_x_continuous(labels=function(x)paste(x/1000, 'k')) +
labs(title = paste('Boxplot for variable:', col)) +
theme_bw()
}
# treemap function
treeMap <- function(col, data, interval, title_, vSize_, vColor_, index_) {
data[interval, ] %>%
mutate_at(c(var = col, index_=index_, ord=vSize_), as.factor) %>%
group_by(var)%>%
arrange(desc(ord))%>%
mutate(col.index=paste(var, index_, sep ="\n"))%>%
treemap(
index="col.index", vSize=vSize_, vColor=vColor_,
palette=c("White","White", "#499894"), # "YlOrRd"
type="value", title.legend=vColor_, title= title_)
}
## 3.1 week
df_week <- as.data.frame(df_subsets[1])
barPlot("Semana", df_week) # There is almost an uniform distribution.
treeMap("Semana", df_week, 1:nrow(df_week),
"Treemap: day percentate", "counts", "counts", "percentOcur")
df_client <- as.data.frame(df_subsets[2])
#head(df_client[, c(1,10,11,12)],10)
boxPlot('NombreCliente', df_client) # there is one client who is an extreme outlier.
# Clients with more ocurrence
df_client %>% arrange(desc(counts))%>%
select(NombreCliente,percentOcur)%>%
head(10)
# this extreme outlier is 'NO IDENTIFICADO'.
# this client is not one customer, it's a group of clients without identification.
# 'NO IDENTIFICADO' is associated with ~17.81% of the records in the dataset.
# 'Lupita' is on the 2nd as the more occurrences.
# however do more occurrences mean more sold units?
df_client %>% arrange(desc(Units))%>%
select(NombreCliente,percentUnit,percentRR, percentSD)%>%
head(10)
# 15,22% of the sold units are for 'NO IDENTIFICADO' clients.
# 'PUEBLA REMISSION' who is the 7-10th in occurrences, it appears here as 2nd.
# so, more occurrences don't mean more sold units, since in one occurrence can have a lot of sold units, and in another occurrence can have only a few sold units.
# 'PUEBLA REMISION' has a return rate highest than 'NO IDENTIFICADO'.
# It's almost proportional the quantity of units sold (percentUnit) to the demand (percentSD).
# Clients with more remained Pesos
df_client %>% arrange(desc(Remaining_Pesos))%>%
select(NombreCliente,Remaining_Pesos, percentRP)%>%
head(10)
# 'NO IDENTIFICADO' shows the highest remained Pesos
#  the more units are sold, the demand increases
# 'PUEBLA REMISION' shows a considered rate compared to others
df_prod <- as.data.frame(df_subsets[3])
boxPlot('Prod_nombre', df_prod)
# There are many outliers in product frequencies
# this group deviates from the sales pattern of other products
treeMap("Prod_nombre", df_prod %>% arrange(desc(counts)),
1:25, "Top 25 Clients with more occurrences",
"counts", "counts", "percentOcur")
treeMap("Prod_nombre", df_prod %>% arrange(desc(Units)),
1:25, "Top 25 Clients for Sold Units ",
"Units", "Sum_Demanda", "percentSD")
# the more units are sold, the demand increases
treeMap("Prod_nombre", df_prod %>% arrange(desc(Remaining_Pesos)),
1:25, "Top 10 remained Pesos",
"Remaining_Pesos", "Sum_Demanda", "percentSD")
# "Pan Blanco" and "Pan Integral" have the highest quantity of remained Pesos
# 'Nito' has relatively more demand than them and than all others, since it has more sold units
df_agencia <- as.data.frame(df_subsets[4])
town <- fread("../Datasets/town_state.csv", header=T)
df_agencia$Town <- town$Town[match(df_agencia$Agencia_ID, town$Agencia_ID)]
df_agencia$State <- town$State[match(df_agencia$Agencia_ID, town$Agencia_ID)]
boxPlot('Agencia_ID', df_agencia)
# there are a few outliers
treeMap("Agencia_ID", df_agencia %>% arrange(desc(counts)),
1:20, "Top 20 Agencias with more occurrences",
"counts", "counts", "percentOcur")
# where are these agencies?
df_agencia %>% arrange(desc(counts))%>%
select(Agencia_ID,percentOcur, State)%>%
head(10)
# ESTADO DE MÉXICO, JALISCO, AGUASCALIENTES states stand out here
# however does more occurrences mean more sold units?
df_agencia %>% arrange(desc(Units))%>%
select(Agencia_ID,percentUnit,percentRR, percentSD, State)%>%
head(10)
# In unit, ESTADO DE MÉXICO appeared less compared to occurrences.
# now it appeared MÉXICO, D.F., it isn't in top 10 occurrences
# JALISCO and AGUASCALIENTES are also here in a stand out way
# 1114 agency has a lower demand percent compared to the percentUnit
# Agencia with more remained Pesos
df_agencia %>% arrange(desc(Remaining_Pesos))%>%
select(Agencia_ID,Remaining_Pesos, percentRP)%>%
head(10)
# 1114 agency has the highest remained Pesos
# 1911 agency that is in the top 1 of the others two lists, it's here as the 5th
df_state <- as.data.frame(df_subsets[6])
boxPlot('State', df_state)
# there are 3 outliers state
# States with more ocurrence
train_sample %>%
group_by(State) %>%
summarise(counts =n()) %>%
mutate(percent = round(prop.table(counts),4)*100) %>%
mutate(percent = paste(percent, "%", sep ="")) %>%
mutate(name = fct_reorder(State, counts)) %>%
ggplot( aes(x=name, y=counts)) +
geom_bar(stat="identity", fill="#f68060", alpha=.6, width=.4) +
geom_text(aes(label = percent), vjust = 1, size = 2.5)+
labs(title = 'The most frequent states:') +
coord_flip() +
xlab("") +
theme_bw()
# "ESTADO DE MÉXICO" is the state of highest frequency
# the tree outliers are ESTADO DE MÉXICO, MÉXICO, D.F. and JALISCO
# Top 10 States with more sold units
df_state %>% arrange(desc(Units))%>%
select(State,percentUnit,percentOcur, percentRR)%>%
head(10)
# 'NUEVO LEÓN' stands out for climbing 3 positions
# Top 10 States with more remained Pesos
df_state %>% arrange(desc(Remaining_Pesos))%>%
select(State,Remaining_Pesos, percentRP, percentUnit)%>%
head(10)
df_town <- as.data.frame(df_subsets[5])
boxPlot('Town', df_town)
# there are two outliers towns
# The 10 most frequent cities
df_town %>% arrange(desc(counts))%>%
select(Town,percentOcur)%>%
head(10)
# these outliers are '2017 AG. SANTA CLARA' and '2309 NORTE'
# Top 10 cities with more sold units
df_town %>% arrange(desc(Units))%>%
select(Town,percentUnit,percentOcur, percentRR)%>%
head(10)
# Top 10 cities with more remained Pesos
df_town %>% arrange(desc(Remaining_Pesos))%>%
select(Town,Remaining_Pesos, percentRP, percentUnit)%>%
head(10)
df_Ruta <- as.data.frame(df_subsets[7])
boxPlot("Ruta_SAK", df_Ruta)
# There are a lot of outliers
# Ruta_SAK with more ocurrences
df_Ruta %>% arrange(desc(counts))%>%
select(Ruta_SAK,percentOcur)%>%
head(10)
# the top 5 is compound by routes between 1201 and 1205
# Top 10 Ruta_SAK with more sold units
df_Ruta %>% arrange(desc(Units))%>%
select(Ruta_SAK,percentUnit,percentOcur, percentRR)%>%
head(10)
# but just 1201 is in top 10 of sold units
# Top 10 Ruta_SAK with more remained Pesos
df_Ruta %>% arrange(desc(Remaining_Pesos))%>%
select(Ruta_SAK,Remaining_Pesos, percentRP, percentUnit)%>%
head(10)
df_channel <- as.data.frame(df_subsets[8])
boxPlot("Canal_ID", df_channel)
# There are two outliers
# Channels occurrences
treeMap("Canal_ID", df_channel %>% arrange(desc(counts)),
1:nrow(df_channel), "Treemap: channels",
"counts", "counts", "percentOcur")
# The two outliers are channels 1 and 4
# Channels with more sold units
df_channel %>% arrange(desc(Units))%>%
select(Canal_ID,Units, percentUnit,percentOcur, percentRR)%>%
head(10)
# channels 2 and 5 have relatively a lot of sold units compared with occurrences
# Top 10 Channel with more remained Pesos
df_channel %>% arrange(desc(Remaining_Pesos))%>%
select(Canal_ID,Remaining_Pesos, percentRP, percentUnit)%>%
head(10)
# the channel 2 has a high remained pesos as well
# for further correlation analysis
train_sample$unitsChannel <- df_channel$Units[match(train_sample$Canal_ID, df_channel$Canal_ID)]
df_unit <- as.data.frame(df_subsets[9])
# distribution data
summary(train_sample$WeekSales_unit) # occurences summary
# top 10 WeekSales_unit occurences
df_unit %>% arrange(desc(counts))%>%
select(WeekSales_unit,counts,percentOcur)%>%
head(10)
# sales with 2 units are the most frequent
# the 1, 2 and 3 units represent 50% of occurrences in dataset
# and the number of the 10 most frequent units varies between 1 and 10.
# top 10 WeekSales_unit sold units
df_unit %>% arrange(desc(Units))%>%
select(WeekSales_unit,Units, percentUnit, percentOcur)%>%
head(10)
# It's interesting, unit 1 appears in the 2nd of the occurrences with 18%, but it does not appear among the 10 most sold units
# now it appears the units 40, 20, and 15 in the top 10
# Top 10 WeekSales_unit with more remained Pesos
df_unit %>% arrange(desc(Remaining_Pesos))%>%
select(WeekSales_unit,Remaining_Pesos, percentRP, percentUnit)%>%
head(10)
# Unit 2 in all the reports stay at the 1st
# almost all the rows are between 1 and 10, except by unit 12 that appeared now, and unit 9 isn't.
df_Pesos <- as.data.frame(df_subsets[10])
# distribution data
summary(train_sample$WeekSales_Pesos) # occurences summary
df_retNexWeek <- as.data.frame(df_subsets[11])
# distribution data
summary(train_sample$ReturnNextWeek_unit) # occurences summary
df_retNexWeekP <- as.data.frame(df_subsets[12])
# distribution data
summary(train_sample$ReturnNextWeek_Pesos) # occurences summary
df_demanda <- as.data.frame(df_subsets[13])
# distribution data
summary(train_sample$Demanda_uni_equil) # quantile(train_sample$Demanda_uni_equil)
boxplot(train_sample$Demanda_uni_equil,horizontal=TRUE,axes=TRUE)
# There are a lot of outliers, let's see the boxplot without them
boxplot(train_sample$Demanda_uni_equil,horizontal=TRUE,axes=TRUE,outline=FALSE)
# df_demanda %>% arrange(desc(counts)) %>%
#   select(Demanda_uni_equil,counts, percentOcur, Units,percentUnit) %>%
#   head(10)
# so let's investigate more about outliers from 3rd quartile
quantile(train_sample$Demanda_uni_equil, probs = c(0.75, 0.8,0.85,0.9,0.95))
# as we can see, the outliers are increasing a lot, especially after 95%
# let's now investigate from 0.95 percentile
quantile(train_sample$Demanda_uni_equil, probs = c(0.95, 0.96,0.97,0.98,0.99, 0.999))
describe(train_sample$Demanda_uni_equil)
# so 96% of the Demanda_uni_equil is  <= 28
train_sample %>% filter(Demanda_uni_equil > 28) %>% tally
# 38957 items are bigger than 28
# df <- df[-which(df$Demanda_uni_equil > 28),] # for to remove data which have Demanda_uni_equil > 28
# let's see the occurrences distribution of these 96%
ggplot(df_demanda, aes(x=Demanda_uni_equil, y = counts))+
geom_histogram(stat = 'identity', alpha = 0.5, fill = '#00A4DEF7')+
scale_x_continuous(name="Demandas", lim=c(0, 29), breaks=0:28)+
scale_y_continuous(name="Frequencia", labels=function(x)paste(x/1000, 'k'))+
theme_bw()
# let's see the sold units distribution of these 96%
ggplot(df_demanda, aes(x=Demanda_uni_equil, y = Units))+
geom_histogram(stat = 'identity', alpha = 0.5, fill = '#00A4DEF7')+
scale_x_continuous(name="Demandas", lim=c(0, 29), breaks=0:28)+
scale_y_continuous(name="Unidades", labels=function(x)paste(x/1000, 'k'))+
theme_bw()
glimpse(train_sample)
# converting columns for numeric intending to correlate variables
sample_num <- train_sample[, c("Semana", "Cliente_ID", "Producto_ID",
"Agencia_ID", "Ruta_SAK", "Canal_ID",
"WeekSales_unit", "WeekSales_Pesos", "ReturnNextWeek_unit",
"ReturnNextWeek_Pesos", "Demanda_uni_equil")]
sample_num[,1:ncol(sample_num)] <- lapply(sample_num[,1:ncol(sample_num)], as.numeric)
glimpse(sample_num)
## correlations
corrplot(cor(sample_num, method="pearson"), method = 'number',
number.cex= 7/ncol(sample_num), type="lower")
# The target variable has value 1 of positively correlation associated with WeekSales_unit (Venta_uni_hoy), so this variable will overfit the model
# The target variable is highly positively associated with WeekSales_Pesos (Venta_hoy)
# The target variable is very little positively associated with Canal_ID
# notice that in test dataset there are just id, Semana, Agencia_ID, Canal_ID, Ruta_SAK, Cliente_ID, Producto_ID columns.
# just Producto_ID and Ruta_SAK are related with target variable, but in a very small correlation, 0.04 and 0.05 respectively
correlation <- sample_num[, c("Cliente_ID", "Producto_ID", "Demanda_uni_equil")]
demand_mean_PC <- correlation %>%
group_by(Producto_ID, Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byPC = mean))
gc() #13GB
correlation <- merge(x=correlation,y=demand_mean_PC,by=c("Producto_ID","Cliente_ID"), all.x = TRUE)
correlation
corrplot(cor(correlation, method="pearson"), method = 'number',
number.cex= 7/ncol(correlation), type="lower")
correlation <- sample_num[, c("Cliente_ID", "Producto_ID", "Agencia_ID", "Canal_ID", "Ruta_SAK", "Demanda_uni_equil")]
demand_mean_PC <- correlation %>%
group_by(Producto_ID, Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byPC = mean))
demand_mean_PC <- correlation %>%
group_by(Producto_ID, Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byPC = mean))
correlation <- merge(x=correlation,y=demand_mean_PC,by=c("Producto_ID","Cliente_ID"), all.x = TRUE)
demand_mean_ACR <- correlation %>%
group_by(Agencia_ID, Canal_ID, Ruta_SAK) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byACR = mean))
correlation <- merge(x=correlation,y=demand_mean_ACR,by=c("Agencia_ID", "Canal_ID", "Ruta_SAK"), all.x = TRUE)
demand_mean_prod <- correlation %>%
group_by(Producto_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byProd = mean))
correlation <- merge(x=correlation,y=demand_mean_prod,by=c("Producto_ID"), all.x = TRUE)
demand_mean_client <- correlation %>%
group_by(Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byClient = mean))
correlation <- merge(x=correlation,y=demand_mean_client,by=c("Cliente_ID"), all.x = TRUE)
demand_mean_Ag <- correlation %>%
group_by(Agencia_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byAg = mean))
correlation <- merge(x=correlation,y=demand_mean_Ag,by=c("Agencia_ID"), all.x = TRUE)
demand_mean_Channel <- correlation %>%
group_by(Canal_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byChannel = mean))
correlation <- merge(x=correlation,y=demand_mean_Channel,by=c("Canal_ID"), all.x = TRUE)
demand_mean_Ruta <- correlation %>%
group_by(Ruta_SAK) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byRuta = mean))
correlation <- merge(x=correlation,y=demand_mean_Ruta,by=c("Ruta_SAK"), all.x = TRUE)
corrplot(cor(correlation, method="pearson"), method = 'number',
number.cex= 7/ncol(correlation), type="lower")
correlation %>% relocate(Demanda_uni_equil, .after = last_col())
corrplot(cor(correlation, method="pearson"), method = 'number',
number.cex= 7/ncol(correlation), type="lower")
correlation <- correlation %>% relocate(Demanda_uni_equil, .after = last_col())
corrplot(cor(correlation, method="pearson"), method = 'number',
number.cex= 7/ncol(correlation), type="lower")
correlation <- correlation[-which(correlation$Demanda_uni_equil > 28),] # for to remove data which have Demanda_uni_equil > 28
corrplot(cor(correlation, method="pearson"), method = 'number',
number.cex= 7/ncol(correlation), type="lower")
---
title: "Data Modelling - Grupo Bimbo Inventory Demand (maximize sales and minimize returns of bakery goods)"
author: "Matheus C. Andrade"
date: "6/29/2021"
output: html_document
---
gc()
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=14)
```
gc()
gc()
knitr::opts_chunk$set(echo = TRUE, fig.width=14)
names(test)
gc()
gc()
## 1.1 Loading 'train' dataset:
train_orig <- fread("../Datasets/train.csv", header=T)
setwd("~/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Grupo-Bimbo-Inventory-Demand/Modelling_and_Evaluation")
getwd()
## Libraries
library(data.table)
library(dplyr)
library(caTools)
library(caret)
library(rpart)
library(caret)
library(pROC)
library(PRROC)
library(xgboost)
## 1.1 Loading 'train' dataset:
train_orig <- fread("../Datasets/train.csv", header=T)
# # # Creating a random sample
# # set.seed(123);
# # train_sample =  train_orig[sample(nrow(train_orig), size=1000000), ] #37090232
# # train_sample <- train_orig
# # rm(train_orig)
# train_sample <- fread("../Datasets/train_sample.csv", header=T)
# 1.2 Loading 'test' dataset:
test <- fread("/home/matheus/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Projeto2/Datasets/test.csv", header=T)
names(test)
# Since the variables Venta_uni_hoy, Venta_hoy, Dev_uni_proxima and Dev_proxima are not present in the test dataset and we won use them for transformation, so we will exclude them from the training dataset.
train_orig <- train_orig %>% select(Semana, Agencia_ID, Canal_ID, Ruta_SAK, Cliente_ID, Producto_ID, Demanda_uni_equil)
## 1.1 Loading 'train' dataset:
#train_orig <- fread("../Datasets/train.csv", header=T)
# # # Creating a random sample
# # set.seed(123);
# # train_sample =  train_orig[sample(nrow(train_orig), size=1000000), ] #37090232
# # train_sample <- train_orig
# # rm(train_orig)
train_orig <- fread("../Datasets/train_sample.csv", header=T)
# 1.2 Loading 'test' dataset:
test <- fread("/home/matheus/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Projeto2/Datasets/test.csv", header=T)
names(test)
# Since the variables Venta_uni_hoy, Venta_hoy, Dev_uni_proxima and Dev_proxima are not present in the test dataset and we won use them for transformation, so we will exclude them from the training dataset.
train_orig <- train_orig %>% select(Semana, Agencia_ID, Canal_ID, Ruta_SAK, Cliente_ID, Producto_ID, Demanda_uni_equil)
# Do you remember in EDA that there is a distortion in the data distribution of the variable Demand_uni_equil?
train_orig %>%
ggplot(aes(x = Demanda_uni_equil)) +
geom_density(fill = '#A6EBC9') +
theme_bw() +
labs(title = 'Density graph for variable: Demanda uni equil') +
xlab('Demanda uni equil')
# So, that's a problem for data modelling.
gc()
gc()
knitr::opts_chunk$set(echo = TRUE, fig.width=14)
setwd("~/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Grupo-Bimbo-Inventory-Demand/Modelling_and_Evaluation")
getwd()
## Libraries
library(data.table)
library(dplyr)
library(caret)
library(xgboost)
# 1.1 Loading 'train' dataset:
#train <- fread("../Datasets/train.csv", header=T)
#train <- fread("../Datasets/train_sample.csv", header=T)
train_orig <- fread("../Datasets/train.csv", header=T)
knitr::opts_chunk$set(echo = TRUE, fig.width=14)
# 1.1 Loading 'train' dataset:
#train <- fread("../Datasets/train.csv", header=T)
train <- fread("../Datasets/train_sample.csv", header=T)
gc()
gc()
library(caret) # for dummyVars
library(RCurl) # download https data
library(Metrics) # calculate errors
library(xgboost) # model
###############################################################################
# Load data from UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets.html)
urlfile <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'
x <- getURL(urlfile, ssl.verifypeer = FALSE)
adults <- read.csv(textConnection(x), header=F)
x <- getURL(urlfile, ssl.verifypeer = FALSE)
adults <- read.csv(textConnection(x), header=F)
# adults <-read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=F)
names(adults)=c('age','workclass','fnlwgt','education','educationNum',
'maritalStatus','occupation','relationship','race',
'sex','capitalGain','capitalLoss','hoursWeek',
'nativeCountry','income')
adults$income
# clean up data
adults$income <- ifelse(adults$income==' <=50K',0,1)
# binarize all factors
library(caret)
adults
View(adults)
# clean up data
adults$income <- ifelse(adults$income==' <=50K',0,1)
# binarize all factors
library(caret)
dmy <- dummyVars(" ~ .", data = adults)
dmy
adultsTrsf <- data.frame(predict(dmy, newdata = adults))
adultsTrsf
# what we're trying to predict adults that make more than 50k
outcomeName <- c('income')
# list of features
predictors <- names(adultsTrsf)[!names(adultsTrsf) %in% outcomeName]
adultsTrsf
# take first 10% of the data only!
trainPortion <- floor(nrow(adultsTrsf)*0.1)
trainSet <- adultsTrsf[ 1:floor(trainPortion/2),]
testSet <- adultsTrsf[(floor(trainPortion/2)+1):trainPortion,]
smallestError <- 100
for (depth in seq(1,10,1)) {
for (rounds in seq(1,20,1)) {
# train
bst <- xgboost(data = as.matrix(trainSet[,predictors]),
label = trainSet[,outcomeName],
max.depth=depth, nround=rounds,
objective = "reg:linear", verbose=0)
gc()
# predict
predictions <- predict(bst, as.matrix(testSet[,predictors]), outputmargin=TRUE)
err <- rmse(as.numeric(testSet[,outcomeName]), as.numeric(predictions))
if (err < smallestError) {
smallestError = err
print(paste(depth,rounds,err))
}
}
}
cv <- 30
trainSet <- adultsTrsf[1:trainPortion,]
cvDivider <- floor(nrow(trainSet) / (cv+1))
smallestError <- 100
for (depth in seq(1,10,1)) {
for (rounds in seq(1,20,1)) {
totalError <- c()
indexCount <- 1
for (cv in seq(1:cv)) {
# assign chunk to data test
dataTestIndex <- c((cv * cvDivider):(cv * cvDivider + cvDivider))
dataTest <- trainSet[dataTestIndex,]
# everything else to train
dataTrain <- trainSet[-dataTestIndex,]
bst <- xgboost(data = as.matrix(dataTrain[,predictors]),
label = dataTrain[,outcomeName],
max.depth=depth, nround=rounds,
objective = "reg:linear", verbose=0)
gc()
predictions <- predict(bst, as.matrix(dataTest[,predictors]), outputmargin=TRUE)
err <- rmse(as.numeric(dataTest[,outcomeName]), as.numeric(predictions))
totalError <- c(totalError, err)
}
if (mean(totalError) < smallestError) {
smallestError = mean(totalError)
print(paste(depth,rounds,smallestError))
}
}
}
setwd("~/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Grupo-Bimbo-Inventory-Demand/EDA")
getwd()
## Libraries
library(data.table)
library(dplyr)
library(caTools)
library(caret)
library(rpart)
library(caret)
library(ROCR,pROC,PRROC)
library(pROC)
library(PRROC)
library(xgboost)
##### Stage 0 - Loading Dataset
train_orig <- fread(file.choose(), header=T) #train <- fread("Datasets/train.csv", header=T)
gc()
## Creating a random sample
set.seed(123);
