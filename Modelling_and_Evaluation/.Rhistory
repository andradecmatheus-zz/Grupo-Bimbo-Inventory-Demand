obj = "reg:squaredlogerror"
for(m in maxDepth) {
for(r in nRounds) {
# Sets a seed to allow the same experiment result to be reproducible:
set.seed(12345)
# Creating the model based on the XGboost algorithm:
model_xgb <- xgb.train(
data      = dTrain,   # defines the predictor variables and the variable to be predicted.
objective = obj,      # regression with squared log loss as learning task
max.depth = m,        # it defines the maximum size of the tree.
nrounds   = r,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, data)
# It stores in a dataframe the parameters used to create the models and their RMSLE metric score obtained:
featuresXGboost <- rbind(featuresXGboost, data.frame(
maxDepth = m,
nRounds  = r,
rmsle    = rmsle(label, pred))
)
# Increases the number of evaluated models.
count <- count + 1
# Print the results during evaluation
print(paste(100 * count / total, '%, best rmsle: ', min(featuresXGboost$rmsle)))
}
}
# Returns the dataframe with the results obtained from each model
featuresXGboost
} # End getBetterXGboostParameters Function
# Generating different models based on the XGboost algorithm and determining their scores for each RMSLE metric.
featuresXGboost <- getBetterXGboostParameters(
data        = as.matrix(train %>% select(- Demanda_uni_equil)),
label       = train$Demanda_uni_equil,
maxDepth    = 12:13,
nRounds     = 84:85
)
# It returns the dataframe with the results obtained from each model:
featuresXGboost
# 1. Loading Dataset
# 1.1 Loading 'train' dataset:
train <- fread("../Datasets/train.csv", header=T)
# 1.2 Loading 'test' dataset:
test <- fread("/home/matheus/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Projeto2/Datasets/test.csv", header=T)
# Since the variables Venta_uni_hoy, Venta_hoy, Dev_uni_proxima and Dev_proxima are not present in the test dataset and we won use them for transformation, so we will exclude them from the training dataset.
train <- train %>% select(Semana, Agencia_ID, Canal_ID, Ruta_SAK, Cliente_ID, Producto_ID, Demanda_uni_equil)
# Calculates the natural logarithm of each value of the variable Demand_uni_equil + 1 unit, that is, log(Demand_uni_equil + 1):
train$Demanda_uni_equil <- log1p(train$Demanda_uni_equil)
# The test dataset has the variables 'id' and 'Demand_uni_equil' that are not contained in the training data and so we will create them for this dataset:
train$id <- 0
train$toTest <- 0
# To distinguish the records that belong to the training dataset from those that belong to the test data, we will create a binary variable named toTest (0: training data; 1:test data):
test$Demanda_uni_equil <- 0
test$toTest <- 1
# for the records of last week, ninth, let's merge them with the record of the test dataset:
data <- rbind(train[Semana == 9], test)
# these data are now in the 'data' object joined with ninth week of training dataset, or this reason we remove it:
rm(test)
gc()
## 2.4 2nd Feature Engineering - creating new predictor variables
# Training data records that were not manipulated will be used in this step:
train <- train[Semana <= 8]
gc()
# Do you remember the correlation stage at EDA? So, now we will create for the model those variables that were better on correlation:
demand_mean_PC <- train %>%
group_by(Producto_ID, Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byPC = mean))
gc()
gc()
data <- merge(x=data,y=demand_mean_PC,by=c("Producto_ID","Cliente_ID"), all.x = TRUE)
gc()
demand_mean_ACR <- train %>%
group_by(Agencia_ID, Canal_ID, Ruta_SAK) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byACR = mean))
data <- merge(x=data,y=demand_mean_ACR,by=c("Agencia_ID", "Canal_ID", "Ruta_SAK"), all.x = TRUE)
demand_mean_prod <- train %>%
group_by(Producto_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byProd = mean))
data <- merge(x=data,y=demand_mean_prod,by=c("Producto_ID"), all.x = TRUE)
demand_mean_client <- train %>%
group_by(Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byClient = mean))
data <- merge(x=data,y=demand_mean_client,by=c("Cliente_ID"), all.x = TRUE)
# removing no longer needed variables:
rm(demand_mean_PC, demand_mean_ACR, demand_mean_prod, demand_mean_client, train)
## 2.5 3rd Feature Engineering - transforming predictor variables
#In this step we will scale the values of the predictor variables between 0 and 1.
gc()
# Defining preprocessing method:
params <- preProcess(data[, !c('id', 'Demanda_uni_equil', 'toTest')], method = 'range')
#  "range" method scales the data to be within rangeBounds. If new samples have values larger or smaller than those in the training set, values will be outside of this range.
gc()
# Transforming the data:
data <- predict(params, data)
## 2.6 Segmenting training and testing data
# Obtaining train record:
train <- data %>%
filter(toTest == 0) %>%
select(-c(id, toTest))
# Obtaining test record:
test <- data %>%
filter(toTest == 1) %>%
select(-c(Demanda_uni_equil, toTest))
# removing no longer needed variables:
rm(data, params)
gc()
gc()
gc()
# Converting the data for  DMatrix type (a dense matrix):
xgb_train <- xgb.DMatrix(data = data.matrix(select(train, -Demanda_uni_equil)), # predictor variables.
label = train$Demanda_uni_equil)                       # variable to be predicted.
# Sets a seed to allow the same experiment result to be reproducible:
set.seed(12345)
gc()
# Sets a seed to allow the same experiment result to be reproducible:
set.seed(12345)
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 12,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 84#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model1_12-34.csv")
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 13,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 85#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model4_13-85.csv")
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 12,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 85#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model3_12-85.csv")
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 13,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 87#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model5_13-87.csv")
gc()
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 14,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 87#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model_14-87.csv")
gc()
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 13,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 86#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model_13-86.csv")
# Sets a seed to allow the same experiment result to be reproducible:
set.seed(12345)
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 13,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 86#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model_13-86_seed.csv")
gc()
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 14,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 86#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model_14-86.csv")
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
set.seed(12345)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 14,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 86#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model_14-86_seed.csv")
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
set.seed(12345)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 13,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 87#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model_13-87_seed.csv")
gc()
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
set.seed(12345)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = 14,#bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = 87#bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, as.matrix(test %>% select(- id)))
# Converting the predicted results to the original scale of the target variable (exp(Demand_uni_equil) - 1):
pred <- expm1(pred)
# Turning any negative prediction into a null value:
pred[pred < 0] <- 0
# Saving the generated results to a CSV file:
d <- data.frame(id = as.integer(test$id), Demanda_uni_equil = pred)
fwrite(d, "html_complete_model_14-87_seed.csv")
setwd("~/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Grupo-Bimbo-Inventory-Demand/Modelling_and_Evaluation")
getwd()
## Libraries
library(data.table)
library(dplyr)
library(caret)
library(xgboost)
library(Metrics) #rmsle function
# 1. Loading Dataset
# 1.1 Loading 'train' dataset:
#train <- fread("../Datasets/train.csv", header=T)
train <- fread("../Datasets/train_sample.csv", header=T)
# 1.2 Loading 'test' dataset:
test <- fread("/home/matheus/Development/DataScienceAcademy/FCD/BigDataRAzure/ProjetoFinal/Projeto2/Datasets/test.csv", header=T)
# 2. Predictive Analytics
## 2.1 Feature Selection
names(test)
# Since the variables Venta_uni_hoy, Venta_hoy, Dev_uni_proxima and Dev_proxima are not present in the test dataset and we won use them for transformation, so we will exclude them from the training dataset.
train <- train %>% select(Semana, Agencia_ID, Canal_ID, Ruta_SAK, Cliente_ID, Producto_ID, Demanda_uni_equil)
#train_orig <- train_orig %>% select(Semana, Agencia_ID, Canal_ID, Ruta_SAK, Cliente_ID, Producto_ID, Demanda_uni_equil)
## 2.2 1st Feature Engineering - transforming the target variable
# Do you remember in EDA that there is a distortion in the data distribution of the variable Demand_uni_equil?
train %>%
ggplot(aes(x = Demanda_uni_equil)) +
geom_density(fill = '#A6EBC9') +
theme_bw() +
labs(title = 'Density graph for variable: Demanda uni equil') +
xlab('Demanda uni equil')
# So, that's a problem for data modelling.
#To reduce irregularity in the data and make the patterns of this variable more visible we will use the log1p (or log(x + 1)) transformation function. And further on, we will also use the function expm1 (or exp(x) - 1) to carry out the inverse process of this transforming.
# Calculates the natural logarithm of each value of the variable Demand_uni_equil + 1 unit, that is, log(Demand_uni_equil + 1):
train$Demanda_uni_equil <- log1p(train$Demanda_uni_equil)
#For more information on how the log function acts on skewed data see this [link](https://onlinestatbook.com/2/transformations/log.html). To better understand the functions log1p and expm1 see this [link](https://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/).
#We emphasize that the main reason for using the log1p function is that there are null values within the variable data, which makes the use of the log function unfeasible because the [log(0) is an undefined value](https://www.rapidtables.com/math/algebra/logarithm/Logarithm_of_0.html).
# Checking for null values for the variable Demanda_uni_equil:
prop.table(table(train$Demanda_uni_equil == 0))
# We find that approximately 1.8% of the data for the variable to be predicted is equal to 0. That's why we applied log1p function to the dataset.
# Visualize the effect of the transformation (log1p) applied on the data:
train %>%
ggplot(aes(x = Demanda_uni_equil)) +
geom_density(fill = '#A6EBC9') +
theme_bw() +
labs(title = 'Density graph for variable: Demanda uni equil') +
xlab('log(Demanda uni equil + 1)')
#We conclude that applying the log1p function decreased the distortion caused by outliers within the dataset of the variable to be predicted and this will help us to achieve better accuracy values in the models we create.
## 2.3 Joining training and testing data in the same dataset
#Our goal in this step is to create a single dataset containing both training data and test data. But, before we perform this action, we should note that there are exclusive variables for each of these datasets.
# The test dataset has the variables 'id' and 'Demand_uni_equil' that are not contained in the training data and so we will create them for this dataset:
train$id <- 0
train$toTest <- 0
# To distinguish the records that belong to the training dataset from those that belong to the test data, we will create a binary variable named toTest (0: training data; 1:test data):
test$Demanda_uni_equil <- 0
test$toTest <- 1
# for the records of last week, ninth, let's merge them with the record of the test dataset:
data <- rbind(train[Semana == 9], test)
# these data are now in the 'data' object joined with ninth week of training dataset, or this reason we remove it:
rm(test)
gc()
## 2.4 2nd Feature Engineering - creating new predictor variables
# Training data records that were not manipulated will be used in this step:
train <- train[Semana <= 8]
gc()
# Do you remember the correlation stage at EDA? So, now we will create for the model those variables that were better on correlation:
demand_mean_PC <- train %>%
group_by(Producto_ID, Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byPC = mean))
gc()
data <- merge(x=data,y=demand_mean_PC,by=c("Producto_ID","Cliente_ID"), all.x = TRUE)
gc()
demand_mean_ACR <- train %>%
group_by(Agencia_ID, Canal_ID, Ruta_SAK) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byACR = mean))
data <- merge(x=data,y=demand_mean_ACR,by=c("Agencia_ID", "Canal_ID", "Ruta_SAK"), all.x = TRUE)
demand_mean_prod <- train %>%
group_by(Producto_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byProd = mean))
data <- merge(x=data,y=demand_mean_prod,by=c("Producto_ID"), all.x = TRUE)
demand_mean_client <- train %>%
group_by(Cliente_ID) %>%
summarise_at(vars(Demanda_uni_equil),
list(Mean_byClient = mean))
data <- merge(x=data,y=demand_mean_client,by=c("Cliente_ID"), all.x = TRUE)
# let's look how the data are:
head(data)
# Note that as we started to evaluate the average values and quantities of each variable by grouping,  the problem of the existence of products within the test data that are not present within the training data has been eliminated.
# removing no longer needed variables:
rm(demand_mean_PC, demand_mean_ACR, demand_mean_prod, demand_mean_client, train)
## 2.5 3rd Feature Engineering - transforming predictor variables
#In this step we will scale the values of the predictor variables between 0 and 1.
gc()
# Defining preprocessing method:
params <- preProcess(data[, !c('id', 'Demanda_uni_equil', 'toTest')], method = 'range')
#  "range" method scales the data to be within rangeBounds. If new samples have values larger or smaller than those in the training set, values will be outside of this range.
gc()
# Transforming the data:
data <- predict(params, data)
# Let's see how the data turned out:
head(data)
## 2.6 Segmenting training and testing data
# Obtaining train record:
train <- data %>%
filter(toTest == 0) %>%
select(-c(id, toTest))
# Obtaining test record:
test <- data %>%
filter(toTest == 1) %>%
select(-c(Demanda_uni_equil, toTest))
# removing no longer needed variables:
rm(data, params)
gc()
## 2.7 Creating XGboost Model
# Function to find the best parameters for XGboost algorithm. It generates various models and compares them.
getBetterXGboostParameters <- function(data, label, maxDepth = 10, nRounds = 100) {
# Creating a dataframe to save model results.
featuresXGboost <- data.frame()
# auxiliary variable to monitoring the progress in the evaluation of the created models.
count <- 0
# it defines the total number of models to be created.
total <- length(maxDepth) * length(nRounds)
# Converting dataset variable data to DMatrix type (a dense matrix).
dTrain <- xgb.DMatrix(
data  = data,      # predictor variables
label = label      # variable to be predicted.
)
# Specify the learning task and the corresponding learning objective (regression with squared log loss):
obj = "reg:squaredlogerror"
for(m in maxDepth) {
for(r in nRounds) {
# Sets a seed to allow the same experiment result to be reproducible:
set.seed(12345)
# Creating the model based on the XGboost algorithm:
model_xgb <- xgb.train(
data      = dTrain,   # defines the predictor variables and the variable to be predicted.
objective = obj,      # regression with squared log loss as learning task
max.depth = m,        # it defines the maximum size of the tree.
nrounds   = r,        # it controls the maximum number of iterations.
)
# Performing prediction with the model based on the XGboost algorithm:
pred <- predict(model_xgb, data)
# It stores in a dataframe the parameters used to create the models and their RMSLE metric score obtained:
featuresXGboost <- rbind(featuresXGboost, data.frame(
maxDepth = m,
nRounds  = r,
rmsle    = rmsle(label, pred))
)
# Increases the number of evaluated models.
count <- count + 1
# Print the results during evaluation
print(paste(100 * count / total, '%, best rmsle: ', min(featuresXGboost$rmsle)))
}
}
# Returns the dataframe with the results obtained from each model
featuresXGboost
} # End getBetterXGboostParameters Function
# Generating different models based on the XGboost algorithm and determining their scores for each RMSLE metric.
featuresXGboost <- getBetterXGboostParameters(
data        = as.matrix(train %>% select(- Demanda_uni_equil)),
label       = train$Demanda_uni_equil,
maxDepth    = 13:14,
nRounds     = 86:87
)
# It returns the dataframe with the results obtained from each model:
featuresXGboost
# The best model is:
featuresXGboost[which.min(featuresXGboost$rmsle), ] #head(sort(featuresXGboost$rmsle, decreasing = F), 1)
# However, after using each of the above settings on kaggle, it was observed that the best result is getting in line 2:
featuresXGboost[2, ]
# However, after using each of the above settings on kaggle, it was observed that the best result is in 2nd row.
featuresXGboost[2, ]
# Therefore, the best model is the 2nd, since models registered after this row show lower performances because they begin overfitting.
bestXGboost <- eaturesXGboost[2, ]
bestXGboost
# Therefore, the best model is the 2nd, since models registered after this row show lower performances because they begin overfitting.
bestXGboost <- featuresXGboost[2, ]
bestXGboost
# Converting the data for  DMatrix type (a dense matrix):
xgb_train <- xgb.DMatrix(data = data.matrix(select(train, -Demanda_uni_equil)), # predictor variables.
label = train$Demanda_uni_equil)                       # variable to be predicted.
# Sets a seed to allow the same experiment result to be reproducible:
set.seed(12345)
# Creating the model based on the XGboost algorithm:
#model_XGB <- xgb.train(data=xgb_train, max.depth=12, nrounds=100)
model_xgb <- xgboost(
data      = xgb_train,   # defines the predictor variables and the variable to be predicted.
#objective = "reg:squaredlogerror", # regression with squared log loss as learning task
max.depth = bestXGboost$maxDepth,        # it defines the maximum size of the tree.
nrounds   = bestXGboost$nRounds,        # it controls the maximum number of iterations.
)
bestXGboost$maxDepth
bestXGboost$nRounds# Creating the model based on the XGboost algorithm:
# It returns the dataframe with the results obtained from each model:
featuresXGboost
# Checking for null values for the variable Demanda_uni_equil:
prop.table(table(train$Demanda_uni_equil == 0))
